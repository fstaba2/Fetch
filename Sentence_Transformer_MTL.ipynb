{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## part1: Sentence Transformer Implementation: This model should be able to encode input sentences into fixed-length embeddings.\n",
    "# Author: Farzaneh Tabataba\n",
    "# Description: Sentence Transformer is a deep learning model to convert sentences into fixed-length vector embeddings. Unlike traditional RNN \n",
    "#              models, that feed the sentence word-by-word to the model, sentence transformers consider the whole sentence at once. We can use\n",
    "#              load the pre-trained models for this purpose and if we have enough data, we can fine-tune the model on our own dataset.\n",
    "#              The most common framework for this purpose is the Hugging Face Transformers library.\n",
    "#              The required libraries for this implementation are listed in the requirements.txt file and installed under venv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model1 = SentenceTransformer('paraphrase-MiniLM-L6-v2') # Small and efficient for paraphrase tasks\n",
    "model2 = SentenceTransformer('paraphrase-mpnet-base-v2') # Higher accuracy for paraphrase identification\n",
    "\n",
    "phrases = [\"King is great.\", \"Queen is his wife.\", \"I love Machine learning.\"]\n",
    "\n",
    "vector_embeddings = model1.encode(phrases)\n",
    "\n",
    "print(vector_embeddings.shape)\n",
    "print(vector_embeddings)\n",
    "\n",
    "vector_embeddings = model2.encode(phrases)\n",
    "print(vector_embeddings.shape)\n",
    "print(vector_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task2: Expand the sentence transformer to handle a multi-task learning setting.\n",
    "# Task A: Sentence Classification, Task B: Sentiment Analysis.\n",
    "# Author: Farzaneh Tabataba\n",
    "\n",
    "# Description: In Multi-Task Learning, we train a single model to learn senrence representation and perform multiple tasks. \n",
    "# Multi-task learning can benefit from having shared low-level features and shared data between multiple tasks. \n",
    "# For NLP tasks, we can use a shared encoder to learn sentence representations and then use different decoders (heads) for different tasks.\n",
    "# For the two tasks of Sentence Classification and Sentiment Analysis, we can use the same encoder to learn sentence representations \n",
    "# and then use two different decoders for output.\n",
    "# Example of TASK A: Sentence Classification: Two-class classification, of positive and negatiive class that sentence belongs to SPORT or not.\n",
    "# TASK B: Sentiment analysis is usually three class classification with labels (positive, negative, neutral). \n",
    "# For both tasks A, B, we can use a Fully connected lasyer (FC) with softmax activation function as the output layer.\n",
    "# The loss function for both tasks can be the cross-entropy loss function.\n",
    "# we assume we have database with labels for all tasks. and we train the model on all data. \n",
    "# However, for the task of Named Entity Recogniton (NER), we need the word-level embedding \n",
    "# and the output for each word would be one of the classes like (person, company, location, etc).\n",
    "# So we use a FC layer with softmax activation function for each word in the sentence.\n",
    "# The loss function is measured for each token with Cross-Entropy loss function. \n",
    "# The total  loss function could be the weighted average of the loss functions for all tasks.\n",
    "# We can train the model on multiple batches of data for each task. \n",
    "# The optimization could be done by Gradient Descent or Adam optimizer for faster convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Training Considerations\n",
    "# Model selection: We can use general purpose pre-trained models like all-MiniLM-L6-v2 which is pretty diverse.\n",
    "# Freezing layers: It really depends on amound of data and the similarity of the tasks. If we have huge amount of data, \n",
    "# we can fune tune the whole model including the transformer layers. But if we have limited data,\n",
    "# We freeze the lower layers of the model, including the Embedding layers and Lower transformer layers,\n",
    "# because they are already well-trained on a large corpus data and shouldn't be updated during fine-tuning.\n",
    "# However, we update the weights for upper transformer layers and the task-specific heads by fine-tuning on each task datasets.\n",
    "# if data is very small and very limited, we freeze all layers of encoder, and only train the task-specific heads.\n",
    "\n",
    "# Metrics for evaluation: For both classificaiton and sentiment analysis we can user Accuracy, Precision, Recall, and F1-score\n",
    "# on the test data. For NER, we can use the F1-score, Precision, and Recall at the token level and for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Training Loop Implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as Func\n",
    "\n",
    "\n",
    "class MultiTaskLearner(nn.Module):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", num_classes=2, sentiment_classes = 3, num_ner_labels=5):\n",
    "        super().__init__()\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        vector_size = self.encoder.get_sentence_embedding_dimension() # Embedding Vector size\n",
    "        \n",
    "        # Task-Specific Heads: Fully connected NN\n",
    "        self.classification_head = nn.Linear(vector_size, num_classes)  # Sentence Classification\n",
    "        self.sentiment_head = nn.Linear(vector_size, sentiment_classes)  # Sentiment (Positive, Neutral, Negative)\n",
    "        self.ner_head = nn.Linear(vector_size, num_ner_labels)  # NER \n",
    "\n",
    "    def forward(self, sentences, task=\"classification\"):\n",
    "        sentence_embeddings = self.encoder.encode(sentences, convert_to_tensor=True)  # Fixed-length embedding\n",
    "        \n",
    "        if task == \"classification\":\n",
    "            return self.classification_head(sentence_embeddings)\n",
    "        elif task == \"sentiment\":\n",
    "            return self.sentiment_head(sentence_embeddings)\n",
    "        elif task == \"ner\":\n",
    "            return self.ner_head(sentence_embeddings)  # Token-level predictions\n",
    "        else:\n",
    "            raise ValueError(\"Task not supported!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(model, training_data, learning_rate=2e-6, epochs=20):\n",
    "    try:\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "        for epoch in range(epochs):\n",
    "            for data in training_data:\n",
    "                sentences, labels, task_type = data  # Training data includes label and task type\n",
    "                \n",
    "                optimizer.zero_grad() # reset gradients\n",
    "                outputs = model(sentences, task=task_type)\n",
    "                \n",
    "                if task_type == \"classification\" or task_type == \"sentiment\":\n",
    "                    # Compute the cross entropy loss between output and labels.\n",
    "                    loss = Func.cross_entropy(outputs, labels)  # outout 2D tensor [batch_size, num_classes]\n",
    "                elif task_type == \"ner\":\n",
    "                    # cross-entropy loss for a (NER) task,  model predicts labels for each token\n",
    "                    # output 3D tensor [batch_size, seq_len, num_labels], for each token we have num_ner_labels probabilities (softmax layer).\n",
    "                    twoD_output = outputs.view(-1, outputs.size(-1)) # [total # of tokens * num_ner_labels = number of possible classes for NER]\n",
    "                    loss = Func.cross_entropy(twoD_output, labels.view(-1))  # reshape labels it into a 1D tensor : batch_size * seq_len\n",
    "                else:\n",
    "                    raise ValueError(\"Task not supported!\")\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f\"Epoch number {epoch+1}: Loss = {loss.item():.3f}\")\n",
    "\n",
    "        print(f\"Training completed after {epochs} epochs!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception during training: {e}\")\n",
    "\n",
    "model = MultiTaskLearner()\n",
    "\n",
    "sentences = [\"Eagles won the game!\", \"bald eagle was seen in the sky today\"]\n",
    "classification_output = model(sentences, task=\"classification\")\n",
    "sentences = [\"Eagles won the game, great job!\", \"Very disappointing game from Kensas city team.\"]\n",
    "sentiment_output = model(sentences, task=\"sentiment\")\n",
    "\n",
    "print(\"Classification Output Shape:\", classification_output.shape)  # [batch_size, num_classes]\n",
    "print(\"Sentiment Output Shape:\", sentiment_output.shape)  # [batch_size, 3]\n",
    "\n",
    "# sample training data\n",
    "training_data = [(\"Eagles won the game!\", 1, \"classification\"), (\"bald eagle was seen in the sky today\", 0, \"classification\"),\n",
    "                 (\"Eagles won the game, great job!\", 1, \"sentiment\"), (\"Very disappointing game from Kensas city team.\", 0, \"sentiment\"),\n",
    "                 (\"Eagles played in New Orleans today!\", [1, 0, 0, 2, 0], \"ner\")]\n",
    "train(model,training_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
